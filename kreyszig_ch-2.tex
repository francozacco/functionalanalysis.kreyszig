\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{adjustbox}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage[mathscr]{euscript}

\title{\textbf{Solved selected problems of Introductory functional analysis with applications - Erwin Kreyszig}}
\author{Franco Zacco}
\date{}

\DeclareSymbolFontAlphabet{\mathscrsfs}{rsfs}
\addtolength{\topmargin}{-3cm}
\addtolength{\textheight}{3cm}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\diam}{\text{diam}}
\newcommand{\cl}{\text{cl}}
\newcommand{\bdry}{\text{bdry}}
\newcommand{\inter}{\text{int}}
\newcommand{\dom}{\mathscrsfs{D}}
\newcommand{\range}{\mathscrsfs{R}}
\newcommand{\nullsp}{\mathscrsfs{N}}

\theoremstyle{definition}
\newtheorem*{solution*}{Solution}

\begin{document}
\maketitle
\thispagestyle{empty}

\section*{Chapter 2 - Normed Spaces. Banach Spaces}
\subsection*{2.1 - Vector Space}
\begin{proof}{\textbf{2}}
    We want to prove (1) and (2). We want to prove first that $0x = \theta$.
    Since $0x$ is a vector we have that $0x + \theta = 0x$
    and $0x + (-0x) = \theta$ hence
    \begin{align*}
        0x + (0x + (-0x)) &= 0x\\
        (0x + 0x) + (-0x) &= 0x\\
        (0 + 0)x + (-0x) &= 0x\\
        0x + (-0x) &= 0x\\
        \theta &= 0x
    \end{align*}
    Therefore we have that $0x = \theta$.

    Now we want to prove that $\alpha\theta = \theta$. Since $\alpha\theta$ is a
    vector we know that $\alpha\theta + \theta = \alpha\theta$ and 
    $\alpha\theta + (-\alpha\theta) = \theta$ hence
    \begin{align*}
        \alpha\theta + (\alpha\theta + (-\alpha\theta)) &= \alpha\theta\\
        \alpha(\theta + \theta) + (-\alpha\theta) &= \alpha\theta\\
        \alpha\theta + (-\alpha\theta) &= \alpha\theta\\
        \theta &= \alpha\theta
    \end{align*}
    Where we used that $\theta + \theta = \theta$ since $\theta$ is also a
    vector. Therefore we have that $\alpha\theta = \theta$.

    Finally, we want to prove that $(-1)x = -x$. We know that
    $(\alpha + \beta)x = \alpha x + \beta x$ using that $\alpha = -1$ and 
    $\beta = 1$ we get that
    \begin{align*}
        (1 + (-1))x &= 1x + (-1)x\\
        0x &= x + (-1)x\\
        \theta &= x + (-1)x\\
        x + (-x) &= x + (-1)x
    \end{align*}
    Where we used that $0x = \theta$ and that $x + (-x) = \theta$.
    This implies that $(-1)x = -x$ as we wanted.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{3}}
    Let $M = \{(1,1,1), (0,0,2)\} \subset \R^3$ so the span of $M$ is given by
    \begin{align*}
        \operatorname{span} M &=
        \{\alpha(1,1,1) + \beta(0,0,2) : \alpha, \beta \in \R \}\\
        &= \{(\alpha,\alpha,\alpha + 2\beta) : \alpha, \beta \in \R \}
    \end{align*}
    Therefore we have that 
    \begin{align*} 
        \operatorname{span} M &= \{(x,y,z) \in \R^3 : x = y\}
    \end{align*}
\end{proof}
\begin{proof}{\textbf{4}}
    We want to check if the following subsets of $\R^3$ constitute
    a subspace of $\R^3$. This is true if given $x,y \in X$ where $X$ is the
    subset in question then $\alpha x + \beta y \in X$ for all scalars
    $\alpha, \beta$.
    \begin{itemize}
        \item [(a)] Let $X$ be "all $x \in \R^3$ such that $\xi_1 = \xi_2$ and
        $\xi_3 = 0$". So if $x, y \in X$ we have that
        \begin{align*}
            \alpha x + \beta y
                &= \alpha (\xi_1, \xi_1, 0) + \beta (\eta_1, \eta_1, 0)\\
                &= (\alpha\xi_1 + \beta\eta_1, \alpha\xi_1 + \beta\eta_1, 0)
        \end{align*}
        We see that if $\alpha x + \beta y = (\mu_1, \mu_2, \mu_3)$ then
        $\mu_1 = \mu_2 = \alpha\xi_1 + \beta\eta_1$ and $\mu_3 = 0$.
        Also we see that $(0, 0, 0) \in X$ as well.
        Therefore we have that $\alpha x + \beta y \in X$ and
        $X$ is a subspace of $\R^3$.
        \item [(b)] Let $X$ be "all $x \in \R^3$ such that $\xi_1 = \xi_2 + 1$".
        So if $x, y \in X$ we have that
        \begin{align*}
            \alpha x + \beta y
                &= \alpha (\xi_2 + 1, \xi_2, \xi_3) + \beta (\eta_2 + 1, \eta_2, \eta_3)\\
                &= (\alpha(\xi_2 + 1) + \beta(\eta_2 + 1),
                \alpha\xi_2 + \beta\eta_2, \alpha\xi_3 + \beta\eta_3)\\
                &= ((\alpha\xi_2 + \beta\eta_2) + \beta + \alpha,
                \alpha\xi_2 + \beta\eta_2, \alpha\xi_3 + \beta\eta_3)
        \end{align*}
        So if $\alpha x + \beta y$ is in $X$ then it must happen that
        $$(\alpha\xi_2 + \beta\eta_2) + \beta + \alpha = (\alpha\xi_2 + \beta\eta_2) + 1$$
        hence it must be that $\alpha + \beta = 1$ which is not true for every
        choice of $\alpha, \beta$. Therefore $\alpha x + \beta y \not\in X$ and
        $X$ is not a subspace of $\R^3$.
        \item [(c)] Let $X$ be "all $x \in \R^3$ such that $\xi_1, \xi_2, \xi_3$
        are positive".
        So if $x, y \in X$ and $\alpha = \beta = -1$ we have that
        \begin{align*}
        \alpha x + \beta y
            &= -1(\xi_1, \xi_2, \xi_3) + -1(\eta_1, \eta_2, \eta_3)\\
            &= (-\xi_1-\eta_1, -\xi_2-\eta_2, -\xi_3-\eta_3)
        \end{align*}
        And since $\eta_i, \xi_i > 0$ then $-\xi_i - \eta_i < 0$ for $i = 1,2,3$.
        Therefore $\alpha x + \beta y \not\in X$ for every $\alpha,\beta$ and
        $X$ is not a subspace of $\R^3$.
        \item [(d)] Let $X$ be "all $x \in \R^3$ such that
        $\xi_1 - \xi_2 + \xi_3 = k$ where $k$ is a constant".
        So if $x, y \in X$ we have that
        \begin{align*}
        \alpha x + \beta y
            &= \alpha(\xi_1, \xi_2, \xi_3) + \beta(\eta_1, \eta_2, \eta_3)\\
            &= (\alpha\xi_1 + \beta\eta_1, \alpha\xi_2+ \beta\eta_2,
            \alpha\xi_3 + \beta\eta_3)
        \end{align*}
        Now, let us compute the following
        \begin{align*}
            (\alpha\xi_1 + \beta\eta_1) -(\alpha\xi_2 + \beta\eta_2) &+
            (\alpha\xi_3 + \beta\eta_3)\\
            &= \alpha(\xi_1 -\xi_2 + \xi_3) + \beta(\eta_1 - \eta_2 + \eta_3)\\
            &= \alpha k + \beta k
        \end{align*}
        Therefore if $\alpha x + \beta y = (\mu_1, \mu_2, \mu_3)$ we see that
        $\mu_1 -\mu_2 + \mu_3 = \alpha k + \beta k$
        so for $\alpha k + \beta k = k$ must happen that $\alpha + \beta = 1$
        for $k \neq 0$ which is not the case for every $\alpha, \beta$.
        Let $k = 0$ and $x = (0,0,0)$ then $0 - 0 + 0 = 0$ so $(0,0,0) \in X$.
        Hence $\alpha x + \beta y \not\in X$ if $k \neq 0$ but
        $\alpha x + \beta y \in X$ if $k = 0$. Finally, $X$ is a subspace of
        $\R^3$ only when $k = 0$.
    \end{itemize}
\end{proof}
\begin{proof}{\textbf{5}}
    Let $\{x_1, x_2, ..., x_n\}$ be a set of $C[a,b]$ where $x_i(t) = t^i$,
    we want to prove that this is a linearly independent set.
    Let us suppose the set is not linearly independent,
    we want to arrive at a contradiction. Then there are
    constants $\alpha_1, \alpha_2, ..., \alpha_n$ where at least one of them 
    is different from $0$ such that
    \begin{align*}
        \alpha_1x_1 + \alpha_2x_2 + ... + \alpha_nx_n &= 0\\
        \alpha_1t^1 + \alpha_2t^2 + ... + \alpha_nt^n &= 0
    \end{align*}
    If $\alpha_n \neq 0$ then this is a polynomial of degree $n$ which has at
    most $n$ roots and hence it cannot be equal to $0$ at every point of $[a,b]$
    then we have a contradiction and must be that $\alpha_n = 0$. We can follow
    the same arguments against any $\alpha_i$ and hence it must be that
    $\alpha_i = 0$ for $i = 1,2,...,n$.
    Therefore the set $\{x_1, x_2, ..., x_n\}$ is a linearly independent set of
    $C[a,b]$.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{6}}
    Let $X$ be an $n$-dimensional vector space and let $x \in X$ then $x$ can
    be expressed as a linear combination of basis vectors $e_1, ..., e_n$,
    we want to show that this representation is unique. Suppose
    there are two representations of $x$ in terms of the basis vectors, we
    want to arrive at a contradiction, then we have that
    \begin{align*}
        x &= \alpha_1e_1 + \alpha_2e_2 + ... + \alpha_n e_n\\
        x &= \beta_1e_1 + \beta_2e_2 + ... + \beta_n e_n
    \end{align*}
    So 
    \begin{align*}
        \alpha_1e_1 + \alpha_2e_2 + ... + \alpha_n e_n
        &= \beta_1e_1 + \beta_2e_2 + ... + \beta_n e_n
    \end{align*}
    and hence
    $$(\alpha_1 - \beta_1)e_1 + (\alpha_2 - \beta_2)e_2 + ...
     + (\alpha_n - \beta_n)e_n = 0$$
    But we know that the basis vectors are a linearly independent set of vectors
    so the coefficients that solve the above equation can only be $0$ i.e.
    $\alpha_i - \beta_i = 0$ for every $i = 1,...,n$
    which implies that $\alpha_i = \beta_i$ and therefore that $x$
    has a unique representation.
\end{proof}
\begin{proof}{\textbf{7}}
    Let $\{e_1, ..., e_n\}$ be a basis for a complex vector space $X$. We want
    to find a basis for $X$ regarded as a real vector space. Let $x \in X$
    then we have that $x$ can be written as 
    \begin{align*}
        x = \alpha_1 e_1 + ... + \alpha_n e_n
    \end{align*}
    Where $\alpha_1, ..., \alpha_n \in \mathbb{C}$ hence each $\alpha_j$ can
    be written as $\alpha_j = a_j + i b_j$ where $a_j, b_j \in \R$ so we can
    write that
    \begin{align*}
        x &= (a_1 + i b_1) e_1 + ... + (a_n + ib_n) e_n\\
          &= a_1e_1 + b_1 ie_1  + ... + a_ne_n + b_n ie_n
    \end{align*}
    Then the set $\{e_1, ..., e_n, ie_1, ..., ie_n\}$ can be defined as a basis
    for $X$ when $X$ is regarded as a real vector space.

    The dimension of $X$ as a complex vector space is $\dim X = n$ and 
    as a real vector space is $\dim X = 2n$.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{10}}
    Let $Y$ and $Z$ be subspaces of a vector space $X$. We want to show that
    $Y \cap Z$ is a subspace of $X$, but $Y \cup Z$ need not be.

    We know that $Y$ and $Z$ are non-empty since they are subspaces of $X$
    then let $y_1, y_2 \in Y$ and $z_1, z_2 \in Z$ and let us take
    $\alpha=\beta= 0$ then $\alpha y_1 + \beta y_2 = 0 \in Y$ and 
    $\alpha z_1 + \beta z_2 = 0 \in Z$ this implies that $0 \in Y \cap Z$
    and thus $Y \cap Z$ is non-empty.

    Let $y_1, z_1 \in Y \cap Z$ and let us compute $\alpha y_1 + \beta z_1$
    where $\alpha, \beta$ are scalars. We know that $y_1, z_1 \in Y$ and 
    that $y_1, z_1 \in Z$ by definition, then $\alpha y_1 + \beta z_1 \in Y$
    and  $\alpha y_1 + \beta z_1 \in Z$ since they are subspaces. But then 
    by the definition of intersection, we have that
    $\alpha y_1 + \beta z_1 \in Y \cap Z$. Therefore $Y \cap Z$ is a subspace
    of $X$.
    
    Let us define $Y = \R$, $Z = i\R$ both considered as subspaces of
    $\mathbb{C}$. Then we see that $Y \cap Z = \{0\}$ which is also
    a subspace of $\mathbb{C}$.
    
    On the other hand, let $1 \in Y$ and $i \in Z$ and let $\alpha=\beta = 1$
    hence we see that $1\cdot 1 + 1\cdot i = 1 + i \not\in Y$ and
    $1 + i \not\in Z$ therefore $1 + i \not\in Y \cup Z$ and thus $Y \cup Z$
    is not a subspace of $\mathbb{C}$.    
\end{proof}
\begin{proof}{\textbf{11}}
    Let $M \neq \emptyset$ be any subset of a vector space $X$,
    we want to prove that $\operatorname{span} M$ is a subspace of $X$.
    Let $m_1, m_2 \in M$ and let $\alpha, \beta$ be scalars.
    We see that $m_1, m_2 \in \operatorname{span} M$
    then $\alpha m_1 + \beta m_2 \in \operatorname{span} M$ since
    $\alpha m_1 + \beta m_2$ is a linear combination of $m_1$ and $m_2$.
    Therefore $\operatorname{span} M$ is a subspace of $X$.
\end{proof}
\subsection*{2.2 - Normed Space. Banach Space}
\begin{proof}{\textbf{1}}
    Let $x \in X$ where $X$ is a normed space then
    the distance from $x$ to $0$ is by definition $d(x, 0)$ hence by the
    definition of the metric induced by the norm we have that
    $d(x, 0) = \|x - 0\| = \|x\|$
\end{proof}
\begin{proof}{\textbf{3}}
    We want to prove that $|\|y\| - \|x\|| \leq \|y - x\|$. From property (N4)
    we have that
    \begin{align*}
        \|(y -x) + x\| \leq \|y - x\| + \|x\|
    \end{align*}
    Hence
    \begin{align*}
        \|y\| -\|x\| \leq \|y - x\|
    \end{align*}
    But also we have that
    \begin{align*}
        \|(y - x) - y\| &\leq \|y - x\| + \|y\|\\
        \|x\| - \|y\|  &\leq  \|y - x\|\\
        -\|y - x\| &\leq \|y\| - \|x\|
    \end{align*}
    Therefore by the properties of the absolute value, we have that
    \begin{align*}
        |\|y\| - \|x\|| \leq \|y - x\|
    \end{align*}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{11}}
    Let $\tilde B(0,1) = \{x \in X: \|x\| \leq 1\}$ be the closed unit ball in
    a normed space $X$, we want to show that $\tilde B(0,1)$ is convex.

    Let $z = \alpha x + (1-\alpha)y$ where $x,y \in \tilde B(0,1)$ and
    $0 \leq \alpha \leq 1$ we want to show that $z \in \tilde B(0,1)$.
    So let us compute the following
    \begin{align*}
        \|z\| &= \|\alpha x + (1-\alpha)y\|\\
            &\leq \|\alpha x\| + \|(1-\alpha)y\|\\
            &= |\alpha|\| x\| + |1 - \alpha|\|y\|\\
            &\leq |\alpha| + |1 - \alpha|
    \end{align*}
    Therefore we see that $\|z\| \leq 1$ hence $z \in \tilde B(0,1)$ and
    in addition $\tilde B(0,1)$ is convex.
\end{proof}
\subsection*{2.3 - Further Properties of Normed Spaces}
\begin{proof}{\textbf{1}}
    Let $c \subset l^\infty$, we want to prove it's a vector subspace of
    $l^\infty$.
    
    Let $x, y \in c$ then they are sequences of the form
    $x = (\xi_j)$ and $y = (\eta_j)$ where each
    $\xi_j$ and $\eta_j$ are complex numbers,
    let also $\alpha, \beta$ be scalars.
    We want to show that $z = \alpha x + \beta y \in c$.
    We see that $z = (\alpha \xi_j + \beta \eta_j)$ is a
    sequence of complex numbers so we are left to prove it's also convergent.

    Suppose $\xi_j \to \xi$ and $\eta_j \to \eta$ we want to
    prove $z$ converges to $\alpha \xi + \beta\eta$ hence we compute
    what follows
    \begin{align*}
        |(\alpha \xi_j + \beta \eta_j) - (\alpha \xi + \beta\eta)|
        &\leq |\alpha \xi_j - \alpha \xi| + |\beta \eta_j- \beta\eta|\\
        &= |\alpha||\xi_j - \xi| + |\beta|| \eta_j - \eta|\\
        &\leq |\alpha|\epsilon + |\beta|\epsilon
    \end{align*}
    Where in the last step we used the fact that both $(\xi_j)$ and $(\eta_j)$
    converge. This implies that
    $\alpha \xi_j + \beta \eta_j \to \alpha \xi + \beta\eta$ and hence that
    $z \in c$, therefore $c$ is a vector subspace of $l^\infty$.

    Now, let $c_0 \subset l^\infty$ be the space of all sequences of scalars
    converging to 0, we want to prove that it's a vector subspace of
    $l^\infty$.

    Let $x,y \in c_0$ then they are sequences of the form
    $x = (\xi_j)$ and $y = (\eta_j)$ such that $\xi_j \to 0$ and $\eta_j \to 0$,
    let also $\alpha, \beta$ be scalars.
    We want to show that $z = \alpha x + \beta y \in c_0$.
    We see that $z = (\alpha \xi_j + \beta \eta_j)$ is a
    sequence of scalars so we are left to prove it's also convergent to 0
    hence let us compute what follows
    \begin{align*}
        |(\alpha \xi_j + \beta \eta_j) - 0|
        &\leq |\alpha \xi_j| + |\beta \eta_j|\\
        &= |\alpha||\xi_j - 0| + |\beta|| \eta_j - 0|\\
        &\leq |\alpha|\epsilon + |\beta|\epsilon        
    \end{align*}
    Where in the last step we used the fact that both $(\xi_j)$ and $(\eta_j)$
    converge to 0. This implies that
    $\alpha \xi_j + \beta \eta_j \to 0$ and hence that
    $z \in c_0$, therefore $c_0$ is a vector subspace of $l^\infty$.

\end{proof}
\cleardoublepage
\begin{proof}{\textbf{2}}
    Let $c_0 \subset l^\infty$ be the space of all sequences of scalars
    converging to 0, we want to prove $c_0$ is closed.
    Let us consider $x = (\xi_j) \in \bar{c}_0$ the closure of $c_0$.
    By Theorem 1.4-6(a) there are  $x_n = (\xi_j^{(n)}) \in c_0$ such that
    $x_n \to x$. Hence, given $\epsilon > 0$, there is an $N \in \N$ such that
    for $n \geq N$ and all $j$ we have that
    \begin{align*}
        |\xi_j^{(n)} - \xi_j| \leq \sup_j |\xi_j^{(n)} - \xi_j| < \epsilon/2
    \end{align*}
    In particular let us take $n = N$ we know that $x_N \in c_0$
    and hence $x_N$ is convergent to $0$ so there is some $N_1 \in \N$
    such that when $j \geq N_1$ we have that
    \begin{align*}
        |\xi_j^{(N)} - 0| < \epsilon/2
    \end{align*}
    Finally, let us apply the triangle inequality to $|\xi_j - 0|$
    when $j \geq N_1$ then
    \begin{align*}
        |\xi_j - 0| = |\xi_j + \xi_j^{(N)} - \xi_j^{(N)}|
        \leq |\xi_j - \xi_j^{(N)}| + |\xi_j^{(N)} - 0| < \epsilon
    \end{align*}
    This implies that $x$ converges to $0$ and that $x \in c_0$ but also since
    $x \in \bar{c}_0$ was arbitrary, this proves that $c_0 \subset l^\infty$
    is closed.
\end{proof}
\begin{proof}{\textbf{3}}
    Let $Y \subset l^\infty$ be the set of all sequences with only finitely
    many nonzero terms. We want to prove it is a subspace of $l^\infty$ but
    not a closed subspace.

    Let $x,y \in Y$ where $x$ has $n$ nonzero terms and $y$ has $m$ nonzero
    terms, let also $\alpha, \beta$ be scalars. We must show that
    $z = \alpha x + \beta y$ is in $Y$ to show that $Y$ is a subspace of
    $l^\infty$. We see that $\alpha x$ still has $n$ nonzero terms where each
    of them is multiplied by the scalar $\alpha$ and the same happens to
    $\beta y$ which has $m$ nonzero terms. Finally, $z = \alpha x + \beta y$
    will have at most $n + m$ nonzero terms so $z$ is in $Y$ and therefore 
    $Y$ is a subspace of $l^\infty$.
    
    Let us consider the sequence $(x_n) \subset Y$ where 
    \begin{align*}
        x_1 &= 1, 0, 0, 0, ...\\
        x_2 &= 1, 1/2, 0, 0, ...\\
        x_3 &= 1, 1/2, 1/3, 0, ...\\
        \vdots
    \end{align*}
    i.e. the first $n$ terms of each element of the sequence have values given
    by $1/n$ and the rest of them are zero.
    We want to prove that $(x_n)$ tends to the sequence $x = (1/n)$.
    Let $\epsilon > 0$ then we can find $N \in \N$ such that when $n \geq N$
    we have that
    $$d(x_n, x) = \sup_i |\xi_i^{(n)} - \xi_i| \leq \frac{1}{N + 1} < \epsilon$$
    Therefore $x_n \to x$ and this implies that $x \in \overline{Y}$ but
    $x \not\in Y$ because it doesn't have finitely many nonzero terms
    hence $Y$ is not closed.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{4}}
    Let $X$ be a normed vector space, we want to prove that vector addition
    and multiplication by a scalar are continuous operations.
    
    The vector addition is a map $(x,y) \to x+y$ from $X \times X$ to $X$ so
    let us define a norm for the space $X \times X$ as 
    \begin{align*}
        \|(x,y)\|_{X\times X} = \max(\|x\|_X, \|y\|_X)
    \end{align*}
    We want to prove that given $\epsilon > 0$ there is some delta $\delta > 0$
    such that when
    $\|(x', y') - (x, y)\|_{X \times X} =
    \|(x'-x, y'-y)\|_{X \times X} < \delta$ we have that
    \begin{align*}
        \|(x' + y') - (x + y)\|_X < \epsilon
    \end{align*}
    We see that
    \begin{align*}
        \|(x' + y') - (x + y)\|_X
            &= \|(x' - x) + (y' - y)\|_X\\
            &\leq \|x' - x\|_X + \|y'- y \|_X\\
            &\leq 2\max(\|x' - x\|_X, \|y' - y\|_X)\\
            &= 2\|(x' - x,y' - y)\|_{X \times X}
    \end{align*}
    Therefore we see that when $\delta = \epsilon/2$ we have that
    $\|(x' - x,y' - y)\|_{X \times X} < \delta$ implies that
    $\|(x' + y') - (x + y)\|_X < \epsilon$ i.e. vector addition is a
    continuous map.

    The multiplication by a scalar is a map $(\alpha, x) \to \alpha x$ from
    $K \times X$ to $X$ so
    let us define a norm for the space $K \times X$ as 
    \begin{align*}
        \|(\alpha, x)\|_{K\times X} = \max(|\alpha|, \|x\|_X)
    \end{align*}
    We want to prove that given $\epsilon > 0$ there is some delta $\delta > 0$
    such that when
    $\|(\beta, x') - (\alpha, x)\|_{K \times X} =
    \|(\beta-\alpha, x'-x)\|_{K \times X} < \delta$ we have that
    \begin{align*}
        \|\beta x' - \alpha x\|_X < \epsilon
    \end{align*}
    We see that
    \begin{align*}
        \|\beta x' - \alpha x\|_X
            &= \|\beta x' - \alpha x + \beta x - \beta x\|_X\\
            &= \|x (\beta - \alpha) + \beta(x' - x)\|_X\\
            &\leq \|x (\beta - \alpha)\|_X  + \|\beta(x' - x)\|_X\\
            &= |\beta - \alpha|\|x\|_X  + |\beta|\|x' - x\|_X
    \end{align*}
    Without loss of generality let us choose $|\beta - \alpha| < 1$
    then we see that
    $$|\beta| \leq |\beta - \alpha|  + |\alpha| < 1 + |\alpha|$$
    Hence we can continue the inequality chain as follows
    \begin{align*}
        \|\beta x' - \alpha x\|_X
            &\leq |\beta - \alpha|\|x\|_X  + |\beta|\|x' - x\|_X\\
            &\leq |\beta - \alpha|\|x\|_X  + (1 + |\alpha|)\|x' - x\|_X\\
            &\leq 2\max(1 + |\alpha|, \|x\|_X)\max(|\beta - \alpha|, \|x' - x\|_X)\\
            &= 2\max(1 + |\alpha|, \|x\|_X)
            \|(\beta - \alpha, x' - x)\|_{K\times X}
    \end{align*}
    Therefore we see that when
    $$\delta = \frac{\epsilon}{2\max(1 + |\alpha|, \|x\|_X)}$$
    we have that
    $$\|(\beta - \alpha,x' - x)\|_{K \times X} < \delta$$ implies that
    $\|\beta x'- \alpha x\|_X < \epsilon$ i.e. multiplication by a scalar is a
    continuous map.
\end{proof}
\begin{proof}{\textbf{5}}
    Let $x_n \to x$ and $y_n \to y$. Let $\epsilon/2 > 0$ then there is
    $N_x, N_y \in \N$ such that when $n \geq N_x$ we have that
    $\|x_n - x\| < \epsilon / 2$ and when $n \geq N_y$ we have that
    $\|y_n - y\| < \epsilon / 2$.
    So by using the triangle inequality and by taking $N = \max(N_x, N_y)$
    then when $n \geq N$ we have that
    \begin{align*}
        \|(x_n + y_n) - (x + y)\| \leq \|x_n - x\| + \|y_n - y\|
        < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
    \end{align*}
    Therefore we see that $x_n + y_n \to x + y$.

    Now let $x_n \to x$ and $\alpha_n \to \alpha$.
    This implies that if we let $\epsilon/2\|x\| > 0$ then there is
    $N_x \in \N$ such that when $n \geq N_x$ we have that
    $\|x_n - x\| < \epsilon/2\|x\|$.
    Also, since $(\alpha_n)$ converges to $\alpha$ it is bounded
    so there is $M > 0$ such that $|\alpha_n| < M$.
    Then if we let $\epsilon/2M > 0$ when $n \geq N_\alpha$ we have that
    $\|\alpha_n - \alpha\| < \epsilon/2M$.

    On the other hand, by using the triangle inequality on 
    $$\|\alpha_n x_n - \alpha x\|
    = \|\alpha_n x_n - \alpha x + \alpha_n x - \alpha_n x\|
    = \|\alpha_n(x_n - x) + x (\alpha_n - \alpha) \|$$
    we see that
    \begin{align*}
        \|\alpha_n(x_n - x) + x (\alpha_n - \alpha) \|
        &\leq \|\alpha_n(x_n - x)\| + \|x (\alpha_n - \alpha) \|\\
        &\leq |\alpha_n|\|x_n - x\| + |\alpha_n - \alpha|\|x \|
    \end{align*}
    Hence by taking $N = \max(N_x, N_\alpha)$ we can continue the inequality
    chain as follows
    \begin{align*}
        \|\alpha_nx_n - \alpha x\|
        &\leq |\alpha_n|\|x_n - x\| + |\alpha_n - \alpha|\|x \|\\
        &< M\|x_n - x\| + |\alpha_n - \alpha|\|x \|\\
        &< M\frac{\epsilon}{2M} + \frac{\epsilon}{2\|x\|} \|x \| = \epsilon
    \end{align*}
    Therefore this implies that $\alpha_n x_n \to \alpha x$.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{6}}
    Let $\overline{Y}$ be the closure of a subspace $Y$ of a normed space $X$
    we want to show that $\overline{Y}$ is a vector subspace.

    Let $x,y \in \overline{Y}$ then there are two sequences
    $(x_n), (y_n) \subseteq Y$ such that $x_n \to x$ and $y_n \to y$.
    Hence, let $\alpha, \beta$ be scalars and let
    $\epsilon/(|\alpha| + |\beta|) > 0$ then there is
    $N_x,N_y \in \N$ such that when $n \geq N_x$ we have that
    $\|x_n - x\| < \epsilon/(|\alpha| + |\beta|)$
    and when $n \geq N_y$ we have that
    $\|y_n - y\| < \epsilon/(|\alpha| + |\beta|)$.

    We want to show that $\alpha x + \beta y$ is also in $\overline{Y}$.
    Since $Y$ is a subspace then the sequence $(\alpha x_n + \beta y_n)$ is in
    $Y$, we want to show that $\alpha x_n + \beta y_n \to \alpha x + \beta y$.

    Let $N = \max(N_x, N_y)$ then using the triangle inequality we see that
    \begin{align*}
        \|(\alpha x_n + \beta y_n) - (\alpha x + \beta y)\|
        &\leq \|\alpha (x_n - x)\| + \|\beta(y_n - y)\|\\
        &\leq |\alpha| \|x_n - x\| + |\beta|\|y_n - y\|\\
        &< \frac{\epsilon}{(|\alpha| + |\beta|)}(|\alpha| + |\beta|) = \epsilon
    \end{align*}
    which implies that $\alpha x_n + \beta y_n \to \alpha x + \beta y$.
    Therefore $\alpha x + \beta y$ is in $\overline{Y}$ and hence $\overline{Y}$
    is also a subspace of $X$.
\end{proof}
\begin{proof}{\textbf{10}}
    Let $X$ be a normed space which has a Schauder basis $(e_n)$.
    We want to prove that there is a subset $M \subseteq X$ which is dense in
    $X$ which would imply that $X$ is separable.

    Let us define
    $$M = \{q_1e_1 + ... + q_n e_n: q_k \in \Q,~ 
    e_k\in (e_n) \text{ and }n\in\N\}$$
    we want to prove $M$ is dense in $X$.

    Let $x \in X$ and let $\epsilon > 0$.
    Since $(e_n)$ is a Schauder basis for $X$ then there is a sequence
    $(\alpha_1e_1 + ... + \alpha_n e_n)$
    where the $\alpha_k$ are scalars such that when $n \geq N$ for some
    $N \in \N$ we have that 
    \begin{align*}
        \|(\alpha_1e_1 + ... + \alpha_n e_n)- x\| < \epsilon/2
    \end{align*}
    Also, let $q_1, ..., q_n \in \Q$ such that
    $|q_k - \alpha_k|\|e_k\|< \epsilon /2n$
    for $1 \leq  k \leq n$ then we have that
    \begin{align*}
        \|(q_1e_1 + ... + q_n e_n) - (\alpha_1e_1 + ... +\alpha_ne_n)\| &\leq
        |q_1 - \alpha_1|\|e_1\| + ...+ |q_n - \alpha_n|\|e_n\| <\\
        &< n (\epsilon / 2n) = \epsilon/2
    \end{align*}
    So let us name $y = (\alpha_1e_1 + ... + \alpha_n e_n)$ then by adding both
    inequalities and applying the triangle inequality
    we have that
    \begin{align*}
        \|(q_1e_1 + ... + q_n e_n) - x\| &\leq
        \|y- x\| + \|(q_1e_1 + ... + q_n e_n) - y\| < \epsilon
    \end{align*}
    This implies that $(q_1e_1 + ... + q_n e_n) \to x$ and hence
    that $x \in \overline{M}$. But $x$ was arbitrary so we have that
    $X = \overline{M}$ and therefore $M$ is a countable dense set
    and $X$ is separable.

\end{proof}
\cleardoublepage
\begin{proof}{\textbf{11}}
    We want to prove that $(e_n) \subset l^p$ where
    $e_n = (\delta_{nj})$ is a Schauder basis for $l^p$.

    Let $x \in l^p$ where $x$ can be written as $(\xi_j)$ then we have that
    \begin{align*}
        \|(\xi_1, \xi_2, ...) &- (\alpha_1 e_1 + ... + \alpha_n e_n)\|_p = \\
        &= \|(\xi_1, \xi_2, ...) - (\alpha_1 (1,0,...) + ... + \alpha_n (0,...,1,0,...))\|_p\\
        &= \|(\xi_1, \xi_2, ...) - (\alpha_1, ..., \alpha_n, 0, 0, ...)\|_p\\
        &= \|((\xi_1 - \alpha_1),..., (\xi_n - \alpha_n), \xi_{n+1}, ...) \|_p
    \end{align*}
    So if we select $\alpha_i = \xi_i$ for $1 \leq i \leq n$ we get that
    \begin{align*}
        \|x - (\alpha_1 e_1 + ... + \alpha_n e_n)\|_p
        &= \|(0, ..., 0, \xi_{n+1}, ...) \|_p
        = \bigg(\sum_{j=n+1}^\infty |\xi_j|^p\bigg)^{1/p}
    \end{align*}
    We know that $\sum_{j=1}^\infty |\xi_j|^p < \infty$, let us suppose it converges to some
    $L$ then
    \begin{align*}
        L = \sum_{j=1}^\infty |\xi_j|^p =
        \sum_{j=1}^n |\xi_j|^p + \sum_{j=n+1}^\infty |\xi_j|^p
    \end{align*}
    Hence
    \begin{align*}
        \sum_{j=n+1}^\infty |\xi_j|^p = L - \sum_{j=1}^n |\xi_j|^p
    \end{align*}
    But $\sum_{j=1}^n |\xi_j|^p \to L$ as $n \to \infty$ then
    $\sum_{j=n+1}^\infty |\xi_j|^p \to 0$ as $n \to \infty$ therefore
    $$\|x - (\alpha_1 e_1 + ... + \alpha_n e_n)\|_p =
    \|(0, ..., 0, \xi_{n+1}, ...) \|_p \to 0$$
    This implies that $(e_n)$ is a Schauder basis for $l^p$.
\end{proof}
\cleardoublepage
\subsection*{2.4 - Finite Dimensional Normed Spaces and Subspaces}
\begin{proof}{\textbf{1}}
    Let $Y \subset l^\infty$ be the set of all sequences with only finitely
    many nonzero terms. We want to prove it is a subspace of $l^\infty$ but
    not a closed subspace.

    Let $x,y \in Y$ where $x$ has $n$ nonzero terms and $y$ has $m$ nonzero
    terms, let also $\alpha, \beta$ be scalars. We must show that
    $z = \alpha x + \beta y$ is in $Y$ to show that $Y$ is a subspace of
    $l^\infty$. We see that $\alpha x$ still has $n$ nonzero terms where each
    of them is multiplied by the scalar $\alpha$ and the same happens to
    $\beta y$ which has $m$ nonzero terms. Finally, $z = \alpha x + \beta y$
    will have at most $n + m$ nonzero terms so $z$ is in $Y$ and therefore 
    $Y$ is a subspace of $l^\infty$.
    
    Let us consider the sequence $(x_n) \subset Y$ where 
    \begin{align*}
        x_1 &= 1, 0, 0, 0, ...\\
        x_2 &= 1, 1/2, 0, 0, ...\\
        x_3 &= 1, 1/2, 1/3, 0, ...\\
        \vdots
    \end{align*}
    i.e. the first $n$ terms of each element of the sequence have values given
    by $1/n$ and the rest of them are zero.
    We want to prove that $(x_n)$ tends to the sequence $x = (1/n)$.
    Let $\epsilon > 0$ then we can find $N \in \N$ such that when $n \geq N$
    we have that
    $$d(x_n, x) = \sup_i |\xi_i^{(n)} - \xi_i| \leq \frac{1}{N + 1} < \epsilon$$
    Therefore $x_n \to x$ and this implies that $x \in \overline{Y}$ but
    $x \not\in Y$ because it doesn't have finitely many nonzero terms
    hence $Y$ is not closed.

    If we now take the same subspace $Y$ but as a subspace of $l^2$ we can
    consider the same sequence $(x_n) \subset Y$ and we can show that 
    $(x_n)$ tends to $x = (1/n)$ as $n \to \infty$ as follows.

    We know that
    \begin{align*}
        d(x_n, x) &= \sqrt{\sum_{i=1}^\infty |\xi_i^{(n)} - \xi_i|^2}\\
        &= \sqrt{\sum_{i=n+1}^\infty |\xi_i|^2}
    \end{align*}
    but also we have by definition that $\sum_{i=1}^\infty |\xi_i|^2 < \infty$
    so let us suppose it converges to some $L$ then
    \begin{align*}
        L = \sum_{i=1}^\infty |\xi_i|^2 = \sum_{i=1}^n |\xi_i|^2
        + \sum_{i=n+ 1}^\infty |\xi_i|^2
    \end{align*}
    Hence
    \begin{align*}
        \sum_{i=n+ 1}^\infty |\xi_i|^2 =  L - \sum_{i=1}^n |\xi_i|^2
    \end{align*}
    But $\sum_{i=1}^n |\xi_i|^2 \to L$ as $n \to \infty$ so
    $\sum_{i=n+ 1}^\infty |\xi_i|^2 \to 0$ which implies that $d(x_n, x) \to 0$
    and that $x_n \to x$. Therefore $x \in \overline{Y}$ but $x \not\in Y$
    thus $Y \subset l^2$ is not closed in $l^2$ either.
\end{proof}
\begin{proof}{\textbf{4}}
    Let $\|\cdot\|$  be a norm on a vector space $X$ such that
    $\|\cdot\|_0$ is an equivalent norm, hence there are $a,b > 0$
    such that $a\|x\| \leq \|x\|_0 \leq b\|x\|$
    for all $x \in X$.
    
    Let $\mathcal{T}_{\|\cdot\|}$ be the topology generated by the norm
    $\|\cdot\|$ over $X$ and let $\mathcal{T}_{\|\cdot\|_0}$ be the topology
    generated by the norm $\|\cdot\|_0$ over $X$.

    Let $U$ be an open set of $\mathcal{T}_{\|\cdot\|}$ then for every $x_0 \in U$
    there is  $B_{\|\cdot\|}(x_0;r) \subseteq U$
    where $B_{\|\cdot\|}(x_0;r)$ is an open ball centered at $x_0$.

    Then by definition, we have that
    \begin{align*}
        B_{\|\cdot\|}(x_0;r) = \{x \in X: \|x - x_0\| < r\}
    \end{align*}
    Given that $x, x_0 \in X$ then $x - x_0 \in X$ so if $\|x - x_0\|_0 < ar$
    then by the equivalence of norms, this implies that
    \begin{align*}
        \|x - x_0\| \leq \frac{1}{a} \|x - x_0\|_0 < r
    \end{align*}
    Hence we see that
    \begin{align*}
        B_{\|\cdot\|_0}(x_0;ar) = \{x \in X: \|x - x_0\|_0 < ar\} \subseteq
        B_{\|\cdot\|}(x_0;r) \subseteq U
    \end{align*}
    So for every $x_0 \in U$ there is $B_{\|\cdot\|_0}(x_0;ar) \subseteq U$
    and hence $U \in \mathcal{T}_{\|\cdot\|_0}$ which implies that
    $\mathcal{T}_{\|\cdot\|} \subseteq\mathcal{T}_{\|\cdot\|_0}$.

    In the same way, if $U$ is an open set of $\mathcal{T}_{\|\cdot\|_0}$
    then for every $x_0 \in U$ there is $B_{\|\cdot\|_0}(x_0;r) \subseteq U$.
    So if $\|x - x_0\| < cr$ then by the equivalence of norms we know that
    $c\|x\|_0 \leq \|x\| \leq d\|x\|_0$ for some scalars $c,d > 0$ and
    this implies that
    \begin{align*}
        \|x - x_0\|_0 \leq \frac{1}{c} \|x - x_0\|_0 < r
    \end{align*}
    Hence we see that
    \begin{align*}
        B_{\|\cdot\|}(x_0;cr) = \{x \in X: \|x - x_0\| < cr\} \subseteq
        B_{\|\cdot\|_0}(x_0;r) \subseteq U
    \end{align*}
    Hence $U \in \mathcal{T}_{\|\cdot\|}$ which implies that
    $\mathcal{T}_{\|\cdot\|_0} \subseteq\mathcal{T}_{\|\cdot\|}$.

    Therefore joining both results we see that 
    $\mathcal{T}_{\|\cdot\|} = \mathcal{T}_{\|\cdot\|_0}$
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{5}}
    Let $\|\cdot\|$  be a norm on a vector space $X$ such that
    $\|\cdot\|_0$ is an equivalent norm, hence there are $a,b > 0$
    such that $a\|x\| \leq \|x\|_0 \leq b\|x\|$ for all $x \in X$.
    In the same way, there are $c,d>0$ such that 
    $c\|x\|_0 \leq \|x\| \leq d\|x\|_0$ for all $x \in X$.

    Let $(x_n) \subseteq (X, \|\cdot\|)$ be a Cauchy sequence then given
    $\epsilon/b > 0$ there is $N \in \N$ such that when $n,m \geq N$
    we have that $\|x_n - x_m\| < \epsilon/b$ but also from the equivalence of
    norms we have that
    \begin{align*}
        \|x_n - x_m\|_0 \leq b\|x_n - x_m\| < \epsilon
    \end{align*}
    So this implies that $(x_n)$ is also Cauchy in $(X, \|\cdot\|_0)$

    In the opposite way if $(x_n) \subseteq (X, \|\cdot\|_0)$ is a Cauchy
    sequence then given $\epsilon/d > 0$ there is $N \in \N$ such that when
    $n,m \geq N$ we have that $\|x_n - x_m\|_0 < \epsilon/d$ but also from the
    equivalence of norms     we have that
    \begin{align*}
        \|x_n - x_m\| \leq d\|x_n - x_m\|_0 < \epsilon
    \end{align*}
    Therefore this implies that $(x_n)$ is also Cauchy in $(X, \|\cdot\|)$

    Finally, joining both results we see that the Cauchy sequences in
    $(X, \|\cdot\|)$ and in $(X, \|\cdot\|_0)$ are the same.
\end{proof}
\subsection*{2.5 - Compactness and Finite Dimension}
\begin{proof}{\textbf{1}}
    By Lemma 2.5-2 we know that a subset $M$ of a metric space is closed
    and bounded so if we see $\R^n$ as a subset of itself we see that 
    $\R^n$ is not bounded so $\R^n$ is not compact. In the same way,
    $\C^n$ is not bounded so $\C^n$ is not compact either.
\end{proof}
\begin{proof}{\textbf{2}}
    Let $X$ be a discrete metric space with infinitely many point.
    Let also $(x_n) \subseteq X$ be a sequence of $X$ such that $d(x_i, x_j) = 1$
    for every $i \neq j$.
    Suppose $(x_{n_k}) \subseteq (x_n)$ is a convergent
    subsequence, we want to arrive at a contradiction.
    
    Given that $(x_{n_k})$ is convergent then $(x_{n_k})$ is also Cauchy.
    Let $\epsilon = 1/2$ then there must be $N \in \N$ such that
    when $k,j \geq N$ we get that
    $$d(x_{n_k}, x_{n_j}) < \epsilon$$
    but this cannot happen since $d(x_{n_k}, x_{n_j}) = 1$ by definition
    for every $k$ and $j$, so we arrived at a contradition.

    Therefore $X$ is not compact.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{3}}
    Let us define a function $f:[0,1] \to \R^2$ as $f(x) = (x, x)$ we see that
    $[0,1]$ is closed and bounded so it is compact because of Theorem 2.5-3
    in addition $f$ is continuous and hence by Theorem 2.5-6 we have
    that $f([0,1])$ is compact.

    On the other hand, let us consider the graph of $x^2$ i.e.
    $\{(x, x^2): x \in \R\}$ then we see that $x^2$ is not bounded so the
    graph of $x^2$ is not compact.
\end{proof}
\begin{proof}{\textbf{5}}
    Let $x \in \R$ then we have the set $[x - 1,x + 1]\subset \R$ which is closed
    and bounded and thus compact. Therefore since $x$ was arbitrary then $\R$
    is locally compact.
    
    Let $z\in \C$ where $z$ has the form $z = a + bi$ for some $a,b\in \R$ 
    then the set
    $$A = \{x + yi : a - 1\leq x \leq a + 1, b - 1\leq y\leq b + 1\}$$
    is closed and bounded and thus compact. Therefore $\C$ is locally compact.

    Let $x \in \R^n$ where $x$ has the form $(x_1, ..., x_n)$ then the set
    \begin{align*}
        \{y \in \R^n: \|x - y\| \leq 1\}
    \end{align*}
    which is a closed ball in $\R^n$ is closed and bounded and thus compact.
    Therefore since $x$ was arbitrary then $\R^n$ is locally compact.

    Let $z \in \C^n$ where $z$ has the form $(z_1, ..., z_n)$ then the set
    \begin{align*}
        \{y \in \C^n: \|z - y\| \leq 1\}
    \end{align*}
    which is a closed ball in $\C^n$ is closed and bounded and thus compact.
    Therefore since $z$ was arbitrary then $\C^n$ is locally compact.
\end{proof}
\begin{proof}{\textbf{6}}
    Let $X$ be a compact metric space so if we take $x \in X$ we see that $X$
    itself is a compact neighborhood of $x$. Therefore $X$ is locally compact.
\end{proof}
\begin{proof}{\textbf{9}}
    Let $X$ be a compact metric space and $M \subseteq X$ be a closed set.
    Let also $(x_n) \subseteq M$ to be a sequence in $M$ then $(x_n)$ has a
    subsequence $(x_{n_k})$ that converges to some $x \in X$ since $X$ is
    compact. But also we know that $M$ is closed so it must happen that
    $x \in M$. Therefore $M$ is compact.
\end{proof}
\begin{proof}{\textbf{10}}
    Let $X$ and $Y$ be metric spaces, $X$ compact, and $T:X \to Y$
    bijective and continuous. We want to show that $T$ is a homeomorphism.

    Let $M \subseteq X$ be a closed set then $M$ is compact because what we
    proved in Problem 9 then $T(M)$ is also compact and therefore closed.
    This implies that $T^{-1}$ is continuous.

    Finally, since $T$ is bijective and continuous and $T^{-1}$ is continuous
    then $T$ is a homeomorphism.
\end{proof}
\cleardoublepage
\subsection*{2.6 - Linear Operators}
\begin{proof}{\textbf{2}}
\begin{itemize}
    \item[-]
    Let $T_1$ be an operator from $\R^2$ to $\R^2$ defined by
    $(\xi_1, \xi_2) \to (\xi_1, 0)$. We want to prove it is linear.
    \begin{itemize}
        \item [(i)] The domain $\mathscrsfs{D}(T_1) = \R^2$ is a vector space
        and the range $\mathscrsfs{R}(T_1) = \R \times \{0\}$ is on the
        vector space $\R^2$ as well.
        \item [(ii)] Let $x,y \in \dom(T_1)$ and $\alpha,\beta$ scalars
        where $x = (\xi_1, \xi_2)$ and $y = (\eta_1, \eta_2)$ then
        \begin{align*}
            T_1(\alpha x + \beta y)
            &= T_1(\alpha(\xi_1, \xi_2) + \beta(\eta_1, \eta_2))\\
            &= T_1((\alpha\xi_1 + \beta\eta_1, \alpha\xi_2 + \beta\eta_2))\\
            &= (\alpha\xi_1 + \beta\eta_1, 0)\\
            &= \alpha(\xi_1, 0) + \beta(\eta_1, 0)\\
            &= \alpha T_1x + \beta T_1y
        \end{align*}
    \end{itemize}
    Therefore $T_1$ is a linear operator. Geometrically $T_1$ projects every
    point to the $x$-axis.

    \item[-]
    Let $T_2$ be an operator from $\R^2$ to $\R^2$ defined by
    $(\xi_1, \xi_2) \to (0, \xi_2)$. We want to prove it is linear.
    \begin{itemize}
        \item [(i)] The domain $\mathscrsfs{D}(T_2) = \R^2$ is a vector space
        and the range $\mathscrsfs{R}(T_2) = \{0\} \times \R$ is on the
        vector space $\R^2$ as well.
        \item [(ii)] Let $x,y \in \dom(T_2)$ and $\alpha,\beta$ scalars
        where $x = (\xi_1, \xi_2)$ and $y = (\eta_1, \eta_2)$ then
        \begin{align*}
            T_2(\alpha x + \beta y)
            &= T_2(\alpha(\xi_1, \xi_2) + \beta(\eta_1, \eta_2))\\
            &= T_2((\alpha\xi_1 + \beta\eta_1, \alpha\xi_2 + \beta\eta_2))\\
            &= (0, \alpha\xi_2 + \beta\eta_2)\\
            &= \alpha(0, \xi_2) + \beta(0, \eta_2)\\
            &= \alpha T_2x + \beta T_2y
        \end{align*}
    \end{itemize}
    Therefore $T_2$ is a linear operator. Geometrically $T_2$ projects every
    point to the $y$-axis.
\cleardoublepage
    \item[-]
    Let $T_3$ be an operator from $\R^2$ to $\R^2$ defined by
    $(\xi_1, \xi_2) \to (\xi_2, \xi_1)$. We want to prove it is linear.
    \begin{itemize}
        \item [(i)] The domain $\mathscrsfs{D}(T_3) = \R^2$ is a vector space
        and the range $\mathscrsfs{R}(T_3) = \R^2$ is a vector space as well.
        \item [(ii)] Let $x,y \in \dom(T_3)$ and $\alpha,\beta$ scalars
        where $x = (\xi_1, \xi_2)$ and $y = (\eta_1, \eta_2)$ then
        \begin{align*}
            T_3(\alpha x + \beta y)
            &= T_3(\alpha(\xi_1, \xi_2) + \beta(\eta_1, \eta_2))\\
            &= T_3((\alpha\xi_1 + \beta\eta_1, \alpha\xi_2 + \beta\eta_2))\\
            &= (\alpha\xi_2 + \beta\eta_2, \alpha\xi_1 + \beta\eta_1)\\
            &= \alpha(\xi_2, \xi_1) + \beta(\eta_2, \eta_1)\\
            &= \alpha T_3x + \beta T_3y
        \end{align*}
    \end{itemize}
    Therefore $T_3$ is a linear operator. Geometrically $T_3$ is a reflection
    over the $y=x$ line.

    \item[-]
    Let $T_4$ be an operator from $\R^2$ to $\R^2$ defined by
    $(\xi_1, \xi_2) \to (\gamma\xi_1, \gamma\xi_2)$.
    We want to prove it is linear.
    \begin{itemize}
        \item [(i)] The domain $\mathscrsfs{D}(T_4) = \R^2$ is a vector space
        and the range $\mathscrsfs{R}(T_4) = \R^2$ is a vector space as well.
        \item [(ii)] Let $x,y \in \dom(T_4)$ and $\alpha,\beta$ scalars
        where $x = (\xi_1, \xi_2)$ and $y = (\eta_1, \eta_2)$ then
        \begin{align*}
            T_4(\alpha x + \beta y)
            &= T_4(\alpha(\xi_1, \xi_2) + \beta(\eta_1, \eta_2))\\
            &= T_4((\alpha\xi_1 + \beta\eta_1, \alpha\xi_2 + \beta\eta_2))\\
            &= (\gamma(\alpha\xi_1 + \beta\eta_1), \gamma(\alpha\xi_2 + \beta\eta_2))\\
            &= \gamma\alpha(\xi_1, \xi_2) + \gamma\beta(\eta_1, \eta_2)\\
            &= \alpha(\gamma\xi_1, \gamma\xi_2) + \beta(\gamma\eta_1, \gamma\eta_2)\\
            &= \alpha T_4x + \beta T_4y
        \end{align*}
    \end{itemize}
    Therefore $T_4$ is a linear operator. Geometrically $T_4$ is a dilation.
\end{itemize}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{3}}
    \begin{itemize}
        \item[-]
        The domain $\dom(T_1)$ of $T_1$ is $\R^2$.\\
        The range $\range(T_1)$ of $T_1$ is $\R \times \{0\}$.\\
        The null space $\nullsp(T_1)$ of $T_1$ is $\{0\} \times \R$.\\

        \item[-]
        The domain $\dom(T_2)$ of $T_2$ is $\R^2$.\\
        The range $\range(T_2)$ of $T_2$ is $\{0\} \times \R$.\\
        The null space $\nullsp(T_2)$ of $T_2$ is $\R \times \{0\}$.\\

        \item[-]
        The domain $\dom(T_3)$ of $T_3$ is $\R^2$.\\
        The range $\range(T_3)$ of $T_3$ is $\R^2$.\\
        The null space $\nullsp(T_3)$ of $T_3$ is $\{(0,0)\}$.
    \end{itemize}
\end{proof}
\begin{proof}{\textbf{5}}
    Let $T:X\to Y$ be a linear operator and let $V\subseteq X$ be a subspace.
    We want to show that $T(V)$ is a vector space.

    Let $x,y\in V$ and $\alpha, \beta$ scalars then $\alpha x + \beta y \in V$
    since $V$ is a subspace.
    We know that $Tx, Ty \in T(V)$ and $T(\alpha x + \beta y) \in T(V)$ as well
    but also we know that $T(\alpha x + \beta y) = \alpha Tx + \beta Ty$
    therefore $\alpha Tx + \beta Ty \in T(V)$ which implies that $T(V)$ is
    a vector space as well.

    Now, let $W \subseteq Y$ be a subspace. We want to prove that $T^{-1}(W)$
    is a vector space on $X$. We know that $T^{-1}(W)$ is defined as
    \begin{align*}
        T^{-1}(W) = \{x \in X : Tx \in W\}
    \end{align*}
    Let $Tx,Ty\in W$ for some $x,y \in X$ and $\alpha, \beta$ scalars then
    $\alpha Tx + \beta Ty \in W$ since $W$ is a subspace.
    We know that $x, y \in T^{-1}(W)$ by definition and since
    $T(\alpha x + \beta y) = \alpha Tx + \beta Ty$ which is in $W$ we see that
    $\alpha x + \beta y \in T^{-1}(W)$.
    This implies that $T^{-1}(W)$ is a vector space on $X$.
\end{proof}
\begin{proof}{\textbf{6}}
    Let $T:X \to Y$ and $S:Y\to Z$ be linear operators and suppose the product
    of these linear operators $ST: X \to Z$ exists. We want to show that $ST$
    is linear.

    We know that $\dom(ST) = X$ is a vector space and $\range(ST)$ lies on 
    the vector space $Z$.

    Let $x,y \in X$ and $\alpha,\beta$ scalars then $\alpha x + \beta y \in X$
    since $X$ is a vector space.
    We know that $T(\alpha x + \beta y) = \alpha Tx + \beta Ty \in Y$ since
    $T$ is a linear operator but also 
    $$S(T(\alpha x + \beta y)) = S(\alpha Tx + \beta Ty)
    = \alpha S(Tx) + \beta S(Ty)$$
    since $S$ is a linear operator. Therefore
    $$ST(\alpha x + \beta y) = \alpha STx + \beta STy$$
    which implies that $ST$ is a linear operator as well.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{7}}
    We want to check if $T_1$ and $T_3$ from Problem 2 commute so let us
    compute the following 
    \begin{align*}
        T_1(T_3(\xi_1, \xi_2)) = T_1(\xi_2, \xi_1) = (\xi_2, 0) 
    \end{align*}
    But
    \begin{align*}
        T_3(T_1(\xi_1, \xi_2)) = T_3(\xi_1, 0) = (0, \xi_1) 
    \end{align*}
    We see that $(T_1T_3)x \neq (T_3T_1)x$ for any $x\in \R^2$ therefore $T_1$
    $T_3$ do not commute.
\end{proof}
\begin{proof}{\textbf{8}}
    We want to write the operators in Problem 2 using 2x2 matrices. Hence
    we have that
    \begin{align*}
        T_1 &= \begin{bmatrix}
            1 & 0\\
            0 & 0
        \end{bmatrix}\qquad
        T_2 = \begin{bmatrix}
            0 & 0\\
            0 & 1
        \end{bmatrix}\\
        T_3 &= \begin{bmatrix}
            0 & 1\\
            1 & 0
        \end{bmatrix}\qquad
        T_4 = \begin{bmatrix}
            \gamma & 0\\
            0 & \gamma
        \end{bmatrix}
    \end{align*}
\end{proof}
\begin{proof}{\textbf{13}}
    Let $T:\dom(T) \to Y$ be a linear operator whose inverse exists.
    Also, let $\{x_1, ..., x_n\}$ be a linearly independent set in $\dom(T)$
    then we know that
    \begin{align*}
        \alpha_1 x_1 + ... + \alpha_n x_n = 0
    \end{align*}
    only when $\alpha_1 = ... = \alpha_n = 0$. Applying $T$ to this equation
    we have that
    \begin{align*}
        T(\alpha_1 x_1 + ... + \alpha_n x_n) &= T0\\
        \alpha_1 Tx_1 + ... + \alpha_n Tx_n &= 0
    \end{align*}
    Where we used that $T0 = 0$ since the inverse of $T$ exists.
    Let us suppose now that $Tx_1, ..., Tx_n$ are not linearly independent,
    then the equation is satisfied for $\alpha_1, ..., \alpha_n$ where not
    all are zeros, we want to arrive at a contradiction.
    Since $T^{-1}$ exists then applying it to this equation gives us
    \begin{align*}
        T^{-1}(\alpha_1Tx_1 + ... + \alpha_n Tx_n) &= T^{-1}0\\
        \alpha_1T^{-1}(Tx_1) + ... + \alpha_n T^{-1}(Tx_n) &= 0\\
        \alpha_1 x_1 + ... + \alpha_n x_n &= 0
    \end{align*}
    But we know that $\{x_1, ..., x_n\}$ are linearly independent so must be
    that $\alpha_i = 0$ for every $1 \leq i \leq n$, a contradiction.
    Therefore $\{Tx_1, ..., Tx_n\}$ are linearly independent.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{14}}
    Let $T: X \to Y$ be a linear operator and $\dim X = \dim Y = n < \infty$
    \begin{itemize}
        \item [($\Rightarrow$)] By the rank-nullity theorem we know that 
        \begin{align*}
            \dim \dom(T) = \dim \range(T) + \dim \nullsp(T)
        \end{align*}
        But also we know that $\range(T) = Y$ then we have that 
        \begin{align*}
            \dim X = \dim Y + \dim \nullsp(T)
        \end{align*}
        Hence must be that $\dim \nullsp(T) = 0$ since $\dim X = \dim Y = n$
        but this implies that the only element of $\nullsp(T)$ is $0$
        therefore $T^{-1}$ exists.

        \item [($\Leftarrow$)] Let us suppose $T^{-1}$ exists 
        then for every $y \in Y$ we have a $T^{-1}(y)$ such that
        \begin{align*}
            T(T^{-1}(y)) = y
        \end{align*}
        Therefore $T$ is surjective and hence $\range(T) = Y$.
    \end{itemize}
\end{proof}
\begin{proof}{\textbf{15}}
    Let $z(t) \in X$ then $z'(t) \in X$ since by definition $z(t)$ have
    derivatives of all orders everywhere on $\R$. But $z(t)$ was arbitrary
    so there is an $x(t)$ for each $x'(t) \in X$ hence $\range(T)$
    is all of $X$ for $T:X \to X$.

    For $T^{-1}$ to exist it must happen that $x'(t) = Tx(t) = 0$ implies that
    $x(t) = 0$ but if we take $x(t) = C$ where $C$ is a constant then
    $x'(t) = 0$ and so an infinite number of functions $x(t) = C$ are sent to
    $x'(t) = 0$. Therefore $T^{-1}$ doesn't exist.
\end{proof}
\cleardoublepage
\subsection*{2.7 - Bounded and Continuous Linear Operators}
\begin{proof}{\textbf{1}}
Let $T_1:X \to Y$ and $T_2: Y \to Z$ then by applying the definition of
bounded operator twice we see that
\begin{align*}
    \|T_1(T_2x)\|\leq \|T_1\|\|T_2x\| \leq \|T_1\|\|T_2\|\|x\|
\end{align*}
Hence dividing by $\|x\|$ and taking the supremum we get that
\begin{align*}
    \sup_{
        \substack{x \in \dom(T_1T_2)\\ x \neq 0}}\frac{\|T_1(T_2x)\|}{\|x\|}
    \leq \|T_1\|\|T_2\|
\end{align*}
Therefore
\begin{align*}
    \|T_1T_2\| \leq \|T_1\|\|T_2\|
\end{align*}
In the same way, let $T:X \to X$ then we see that
\begin{align*}
    \|T^n(x)\| = \|T(T^{n-1}(x))\|\leq \|T\|\|T^{n-1}(x)\|
\end{align*}
So Applying the bounded operator definition $n$ times we get that
\begin{align*}
    \|T^n(x)\| \leq \|T\|\|T\| ... \|T\|\|x\| = \|T\|^n\|x\|
\end{align*}
Hence dividing by $\|x\|$ and taking the supremum we get that
\begin{align*}
    \sup_{
        \substack{x \in \dom(T^n)\\ x \neq 0}}\frac{\|T^n(x)\|}{\|x\|}
    \leq \|T\|^n
\end{align*}
Therefore
\begin{align*}
    \|T^n\| \leq \|T\|^n
\end{align*}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{2}}
    Let $X$ and $Y$ be normed spaces and let $T:X \to Y$ to be a linear
    operator.
    \begin{itemize}
        \item [($\Rightarrow$)] Let $T$ be bounded and let $D\subseteq \dom(T)$
        be a bounded subset in $X$ then there is a scalar $d$ such that
        for every $x \in D$ we have that $\|x\| \leq d$. Also, we know that
        there is a scalar $c$ such that $\|Tx\| \leq c\|x\|$ for every $x$ in
        $D$ since $T$ is bounded. Hence
        \begin{align*}
            \|Tx\| \leq c\|x\| \leq cd
        \end{align*}
        which implies that the subset $M = \{Tx: x \in D\}$ is a bounded subset
        of $Y$. Therefore $T$ takes bounded subsets in $X$ to bounded subsets
        in $Y$.
        \item [($\Leftarrow$)] Let $T$ map bounded sets in $X$ into bounded
        sets in $Y$. Then let $D \subseteq X$ be a bounded subset such that
        $\|x\| = 1$ for every $x \in D$ then $T(D)$ is a bounded subset of $Y$,
        hence $\|Tx\| \leq c$ for a scalar $c$ and for every $Tx \in T(D)$.
        
        So we have that there is some $\|T\|$ such that
        \begin{align*}
            \|T\| = \sup_{\substack{x \in \dom(T)\\ \|x\| = 1}} \|Tx\|
        \end{align*}
        Given that we can find the norm of $T$ and we know by definition that
        also
        \begin{align*}
            \|T\| = \sup_{\substack{x \in \dom(T)\\ x \neq 0}} \frac{\|Tx\|}{\|x\|}
        \end{align*}
        then
        \begin{align*}
            \|Tx\| \leq \|T\|\|x\|
        \end{align*}
        Which implies that $T$ is bounded.
    \end{itemize}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{5}}
    Let $T: l^\infty \to l^\infty$ such that $Tx = (\xi_j/j)$ where
    $x = (\xi_j)$. We want to show it's linear and bounded.

    Let $z = \alpha x + \beta y \subseteq l^\infty$ be a sequence such that
    $x= (\xi_j)$ and $y = (\eta_j)$then
    \begin{align*}
        T(\alpha x + \beta y) &= T(\alpha(\xi_j) + \beta(\eta_j))\\
            &=\frac{\alpha(\xi_j) + \beta(\eta_j)}{j}\\
            &=\alpha\bigg(\frac{\xi_j}{j}\bigg) + \beta\bigg(\frac{\eta_j}{j}\bigg)\\
            &=\alpha Tx + \beta Ty 
    \end{align*}
    Therefore $T$ is linear.

    On the other hand, we see that
    \begin{align*}
        \sup_j \bigg|\frac{\xi_j}{j}\bigg| \leq \sup_j |\xi_j|
    \end{align*}
    So we have that
    \begin{align*}
        \|Tx\|_\infty \leq \|x\|_\infty
    \end{align*}
    Which implies that $T$ is bounded.    
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{6}}
    Let us take the linear operator $T:l^\infty \to l^\infty$ defined in
    Problem 5 then the range of $T$ is
    \begin{align*}
        \range(T) = \{(\xi_j/j) : (\xi_j) \in l^\infty\}
    \end{align*}
    Let us take the following sequence of sequences
    \begin{align*}
        &\bigg(1, 0, 0, 0, ...\bigg)\\
        &\bigg(1, \frac{\sqrt{2}}{2}, 0, 0, ...\bigg)
        = \bigg(1, \frac{1}{\sqrt{2}}, 0, 0, ...\bigg)\\
        &\bigg(1, \frac{\sqrt{2}}{2}, \frac{\sqrt{3}}{3}, 0, ...\bigg)
        = \bigg(1, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{3}}, 0, ...\bigg)\\
        & \qquad\vdots
    \end{align*}
    We see that all of them are in $\range(T)$ but this sequence of
    sequences tend to
    $$\bigg(1, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{3}}, \frac{1}{\sqrt{4}},
    ...\bigg) \not \in \range(T)$$
    Therefore $\range(T)$ is not closed.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{7}}
    Let $T$ be a bounded linear operator from a normed space $X$ onto a normed
    space $Y$. If there is a positive $b$ such that
    \begin{align*}
        \|Tx\| \geq b \|x\| \quad \text{ for all }x\in X
    \end{align*}
    We want to show that $T^{-1}: Y \to X$ exists and is bounded.

    Let $Tx = 0$ then
    \begin{align*}
        0 \geq b \|x\|
    \end{align*}
    which implies that $\|x\| = 0$ but then by the norms definition we have
    that $x = 0$. Therefore $T^{-1}$ exists by Theorem 2.6-10.
    
    Let $y \in Y$ then there is $T^{-1}y \in X$ then we have that
    \begin{align*}
        \|y\| \geq b \|T^{-1}y\|
    \end{align*}
    Hence since $b > 0$ we get that
    \begin{align*}
        b \|T^{-1}y\| &\leq \|y\|\\
        \|T^{-1}y\| &\leq \frac{1}{b}\|y\|
    \end{align*}
    Which implies that $T^{-1}$ is bounded.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{8}}
    Let $T: l^\infty \to l^\infty$ defined by $Tx = (\xi_j/j)$ where
    $x = (\xi_j)$. If $Tx = (0)$ then must be that $x = (0)$, hence $T^{-1}$
    exists.

    Suppose $T^{-1}$ is bounded we want to arrive at a contradiction. Let
    \begin{align*}
        y = (1, 1, 1, ..., 1, 0, 0, ...)
    \end{align*}
    i.e. $y$ has $n$ ones. Then must be that
    \begin{align*}
        T^{-1}y = (1, 2, 3, ..., n, 0, 0, ...)
    \end{align*}
    Hence
    \begin{align*}
        \|y\| &= \sup_j |\xi_j/j| = 1\\
        \|T^{-1}y\| &= \sup_j |\xi_j| = n
    \end{align*}
    So if $T^{-1}$ were bounded we have that
    \begin{align*}
        \|T^{-1}y\| \leq c\|y\|\\
        n \leq c
    \end{align*}
    But if we pick $c = n + 1$ then it's not going to work for a
    $y = (1, 1, ..., 1, 1, 0, 0, ...)$ with $n + 1$ ones therefore
    we have a contradiction and $T^{-1}$ cannot be bounded.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{9}}
    Let $T: C[0,1] \to C[0,1]$ defined by
    \begin{align*}
        y(t) = \int_0^t x(\tau)~d\tau
    \end{align*}
    Let $y(t)$ be a differentiable function such that $y(0) = 0$ we want to show
    there is some $x(t)$ such that
    \begin{align*}
        y(t) = \int_0^t x(\tau)~d\tau
    \end{align*}
    Let us take $x(t) = y'(t)$ then we have that
    \begin{align*}
        \int_0^t y'(\tau)~d\tau = y(t) - y(0) = y(t)
    \end{align*}
    Since $y(t)$ is the antiderivative of $x(t)$ by definition.
    Then $\range(T)$ is every continuous function $f:[0,1] \to \R$ such that
    $f$ is differentiable and $f(0) = 0$.

    The inverse $T^{-1}$ is the map that takes continuous functions
    $f:[0,1] \to \R$ to their derivatives
    $f':[0,1] \to \R$ hence $T^{-1}:\range(T) \to C[0,1]$ is defined as
    \begin{align*}
        T^{-1}y(t) = y'(t)
    \end{align*}
    $T^{-1}$ is the differentiation operator and we saw that this operator is
    linear but not bounded in Example 2.7-5.
\end{proof}
\cleardoublepage
\subsection*{2.8 - Linear Functionals}
\begin{proof}{\textbf{2}}
    Let the functional
    \begin{align*}
        f_1(x) = \int_a^b x(t)y_0(t)~dt
    \end{align*}
    Where $y_0 \in C[a,b]$.\\
    Let $x,y \in C[a,b]$ and $\alpha, \beta$ be scalars then
    \begin{align*}
        \alpha f_1(x) + \beta f_1(y)
        &= \alpha \int_a^b x(t)y_0(t)~dt + \beta\int_a^b y(t)y_0(t)~dt\\
        &= \int_a^b \alpha x(t)y_0(t) + \beta y(t)y_0(t)~dt\\
        &= \int_a^b (\alpha x(t) + \beta y(t))y_0(t)~dt\\
        &= f_1(\alpha x + \beta y)
    \end{align*}
    Then $f_1$ is linear.\\
    On the other hand, we see that
    \begin{align*}
        |f_1(x)|
        = \bigg|\int_a^b x(t)y_0(t)~dt\bigg|
        \leq (b-a)\max_{t\in[a,b]} |x(t)| \max_{t \in [a,b]} |y_0(t)|
        =(b-a)\|x\|\|y_0\|
    \end{align*}
    Given that $\|y_0\|$ is a constant then taking $c = (b-a) \|y_0\|$ we get
    that
    \begin{align*}
        |f_1(x)| \leq (b-a)\|y_0\|\|x\|
    \end{align*}
    Therefore $f_1$ is bounded.

    Let now the functional
    \begin{align*}
        f_2(x) = \alpha x(a) + \beta x(b)
    \end{align*}
    Where $\alpha, \beta$ are fixed.\\
    Let $x, y \in C[a,b]$ and $\delta, \eta$ be scalars then
    \begin{align*}
        \delta f_2(x) + \eta f_2(y)
        &= \delta(\alpha x(a) + \beta x(b)) + \eta(\alpha y(a) + \beta y(b))\\
        &= \alpha (\delta x(a) + \eta y(a)) + \beta (\delta x(b) + \eta y(b))\\
        &= f_2(\delta x + \eta y)
    \end{align*}
    Then $f_2$ is linear.\\
    On the other hand, we see that
    \begin{align*}
        |f_2(x)| = |\alpha x(a) + \beta x(b)|
        &\leq |\alpha||x(a)| + |\beta||x(b)|\\
        &\leq (|\alpha| + |\beta|)\max(|x(a)|, |x(b)|)\\
        &\leq (|\alpha| + |\beta|)\max_{t \in C[a,b]}|x(t)|
        = (|\alpha| + |\beta|)\|x\|
    \end{align*}
    Therefore taking $c = |\alpha| + |\beta|$ we see that $f_2$ is bounded.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{4}}
    Let the functional
    \begin{align*}
        f_1(x) = \max_{t \in J} x(t)
    \end{align*}
    Where $J = [a, b]$.\\
    Let $a = 0$, $b = 1$ and 
    \begin{align*}
        x(t) = t \text{ and } y(t) = -t
    \end{align*}
    Also, let $\alpha = \beta = 1$ then
    \begin{align*}
        \alpha f_1(x) + \beta f_1(y)
        &= \max_{t \in [0,1]} x(t) + \max_{t \in [0, 1]} y(t)
        = 1 + 0 = 1
    \end{align*}
    But
    \begin{align*}
        f_1(\alpha x + \beta y)
        &= \max_{t \in [0,1]} (t - t) = 0
    \end{align*}
    Therefore $\alpha f_1(x) + \beta f_1(y) \neq f_1(\alpha x + \beta y)$
    and hence $f_1$ is not linear.\\
    On the other hand, we see that
    \begin{align*}
        |f_1(x)| = |\max_{t \in J} x(t)| \leq \max_{t \in J} |x(t)| = \|x(t)\|
    \end{align*}
    then taking $c = 1$ we get that
    \begin{align*}
        |f_1(x)| \leq \|x(t)\|
    \end{align*}
    Therefore $f_1$ is bounded.

    Let now the functional
    \begin{align*}
        f_2(x) = \min_{t \in J} x(t)
    \end{align*}
    Where $J = [a, b]$.\\
    Let $a = 0$, $b = 1$ and 
    \begin{align*}
        x(t) = t \text{ and } y(t) = -t
    \end{align*}
    Also, let $\alpha = \beta = 1$ then
    \begin{align*}
        \alpha f_2(x) + \beta f_2(y)
        &= \min_{t \in [0,1]} x(t) + \min_{t \in [0, 1]} y(t)
        = 0 + (-1) = -1
    \end{align*}
    But
    \begin{align*}
        f_2(\alpha x + \beta y)
        &= \min_{t \in [0,1]} (t - t) = 0
    \end{align*}
    Therefore $\alpha f_2(x) + \beta f_2(y) \neq f_2(\alpha x + \beta y)$
    and hence $f_2$ is not linear.\\
    On the other hand, we see that
    \begin{align*}
        |f_2(x)| = |\min_{t \in J} x(t)| \leq \max_{t \in J} |x(t)| = \|x(t)\|
    \end{align*}
    then taking $c = 1$ we get that
    \begin{align*}
        |f_2(x)| \leq \|x(t)\|
    \end{align*}
    Therefore $f_2$ is bounded.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{5}}
    Let $f(x) = \xi_n$ ($n$ fixed) where $x = (\xi_j)$, we want to prove that
    $f$ is a linear functional.
    Let $x = (\xi_j)$, $y = (\eta_j)$ and $\alpha, \beta$ scalars then
    \begin{align*}
        f(\alpha x + \beta y) = \alpha \xi_n + \beta \eta_n
        = \alpha f(x) + \beta f(y)
    \end{align*}
    So $f$ is a linear operator and since the domain of $f$ is the sequence
    space $X$ and the range of $f$ is in the scalar field $K$ of $X$ then
    $f$ defines a linear functional.\\
    Let now $X = l^\infty$ then we see that
    \begin{align*}
        |f(x)| = |\xi_n| \leq \sup_j |\xi_j| = \|x\|_\infty
    \end{align*}
    Therefore $f$ is bounded.
\end{proof}
\begin{proof}{\textbf{11}}
    Let $f_1 \neq 0$ and $f_2 \neq 0$ be two linear functionals, they are
    defined on a vector space $X$ and they have the same null space.\\
    Let $z \not\in \nullsp(f_1)$ then there is some $c$ such that
    $f_1(z) = c f_2(z)$.\\
    Let also $x \in X$ such that $x \neq z$, first we prove that $x$ can be
    written as $x = az + w$ where $a$ is some scalar and
    $w \in \nullsp(f_1)$, note that
    \begin{align*}
        f_1(x) = f_1(az + w) = af_1(z) + f_1(w) = af_1(z)
    \end{align*}
    So it's enough to define $a = f_1(x)/f_1(z)$.
    On the other hand, let us define $w = x - az$, we want to prove that $w$
    is in the null space
    \begin{align*}
        f_1(w) = f_1(x - az) = f_1(x) - af_1(z) = af_1(z) - af_1(z) = 0
    \end{align*}
    then $w$ is in the null space and $x$ can be written as we want.
    Then we can write that
    \begin{align*}
        f_1(x) = af_1(z) + f_1(w) = acf_2(z) + f_2(w) = cf_2(az + w) = cf_2(x) 
    \end{align*}
    Therefore $f_1$ and $f_2$ are proportional.
\end{proof}
\cleardoublepage
\subsection*{2.9 - Linear Operators and Functionals on Finite Dimensional Spaces}
\begin{proof}{\textbf{3}}
    The dual basis of $\{(1,0,0), (0,1,0), (0,0,1)\} \in \R^3$ is
    $F = \{f_1, f_2, f_3\}$ where
    \begin{align*}
        f_1 = (1, 0, 0) \quad f_2 = (0,1,0) \quad f_3 = (0,0,1)
    \end{align*}
    Since
    \begin{align*}
        f_k(e_j) = \begin{cases}
            0 \quad\mbox{if } j \neq k\\
            1 \quad\mbox{if } j = k
        \end{cases}
    \end{align*}
\end{proof}
\begin{proof}{\textbf{4}}
    Let $F = \{f_1, f_2, f_3\}$ be the dual basis of $\{e_1, e_2, e_3\}$ for
    $\R^3$ where $e_1 = (1,1,1)$, $e_2 = (1,1,-1)$ and $e_3 = (1,-1,-1)$.
    For $F$ to be a dual basis must happen that
    \begin{align*}
        f_k(e_j) = \begin{cases}
            0 \quad\mbox{if } j \neq k\\
            1 \quad\mbox{if } j = k
        \end{cases}
    \end{align*}
    For $f_1$ then must happen that
    \begin{align*}
        1\cdot \alpha_1 + 1 \cdot \alpha_2 + 1 \cdot \alpha_3 &= 1\\
        1\cdot \alpha_1 + 1 \cdot \alpha_2 - 1 \cdot \alpha_3 &= 0 \\
        1\cdot \alpha_1 - 1 \cdot \alpha_2 - 1 \cdot \alpha_3 &= 0 
    \end{align*}
    The solution to this set of equations imply that $\alpha_1 = 1/2$,
    $\alpha_2 = 0$ and $\alpha_3 = 1/2$ i.e. $f_1 = (1/2, 0, 1/2)$.
    In the same way, we can find $f_2 = (0, 1/2, -1/2)$ and
    $f_3 = (1/2, -1/2, 0)$.\\
    Finally, let $x = (1, 0, 0)$ then
    \begin{align*}
        f_1(x) &= 1 \cdot 1/2 + 0\cdot 0 + 0 \cdot 1/2 = 1/2\\
        f_2(x) &= 1 \cdot 0 + 0\cdot 1/2 - 0 \cdot 1/2 = 0\\
        f_3(x) &= 1 \cdot 1/2 - 0\cdot 1/2 + 0 \cdot 0 = 1/2
    \end{align*}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{5}}
    Let $f:X \to K$ be a linear functional on an n-dimensional vector space $X$.
    By the Rank-Nullity Theorem we know that
    $$\dim(\nullsp(f)) + \dim(\range(f)) = \dim(\dom(f))$$
    If $f$ is not the zero functional then $\dim(\range(f)) = 1$ and since we
    know that $\dim(\dom(f)) = n$ then must be that $\dim(\nullsp(f)) = n - 1$.

    On the other hand, if $f$ is the zero functional then $\dim(\range(f)) = 0$
    hence $\dim(\nullsp(f)) = n$.

    Therefore $\dim(\nullsp(f))$ can be of dimension $n$ or $n - 1$.     
\end{proof}
\begin{proof}{\textbf{6}}
    Let $f(x) = \xi_1 + \xi_2 - \xi_3$ we want to find a basis for the null
    space of $f$ this implies we want to find $\{e_1, e_2\} \subset \R^3$ such
    that $f(e_1) = f(e_2) = 0$ where $e_1$ and $e_2$ are linearly
    independent.
    
    Let $e_1 = (0, 1, 1)$ and $e_2 = (1, 0, 1)$ we see they are linearly
    independent and
    \begin{align*}
        f(e_1) &= 0 + 1 - 1 = 0\\
        f(e_2) &= 1 + 0 - 1 = 0
    \end{align*}
    Therefore they are a basis for the null space of $f$.
\end{proof}
\begin{proof}{\textbf{13}}
    Let $B_Z = \{e_1, ..., e_m\} \subset Z$ be a basis for $Z$ where $m < n$
    then we know that we can extend $B_Z$ with $\{e_{m+1},..., e_n\}$
    to get a basis for $X$.
    
    Then let us define a linear functional $\tilde{f}$ as
    $\tilde{f}(e_i) = f(e_i)$ when $1 \leq i \leq m$ and $\tilde{f}(e_i) = 0$
    when $m + 1\leq i \leq n$.

    Now, let $x \in X$ then $x$ can be written as
    $x = \sum_{i=1}^{m} \alpha_i e_i + \sum_{j=m + 1}^{n} \alpha_j e_j$
    and hence
    \begin{align*}
        \tilde{f}(x)
        = \sum_{i=1}^{m} \alpha_i \tilde{f}(e_i)
        + \sum_{j=m + 1}^{n} \alpha_j \tilde{f}(e_j)
        = \sum_{i=1}^{m} \alpha_i f(e_i)
    \end{align*}
    And we see that if $x \in Z$ then
    \begin{align*}
        \tilde{f}(x)
        = \sum_{i=1}^{m} \alpha_i \tilde{f}(e_i)
        = \sum_{i=1}^{m} \alpha_i f(e_i)
    \end{align*}
    Therefore $\tilde{f}$ and $f$ agree on $x \in Z$.
\end{proof}
\cleardoublepage
\subsection*{2.10 - Normed Spaces of Operators. Dual Space}
\begin{proof}{\textbf{1}}
    Let $T \in B(X,Y)$ and let us call the zero element of the vector space
    $B(X,Y)$ as $\theta \in B(X,Y)$ then $\theta:X \to Y$ must take any element
    of $x \in X$ to the zero element in $Y$ let us call it $0 \in Y$ so
    we have that
    \begin{align*}
        (T + \theta)x = Tx + \theta x = Tx + 0 = Tx
    \end{align*}
    Hence $\theta$ has the property $T + \theta = T$ for any $T \in B(X,Y)$.

    On the other hand, let us define the additive inverse of $T$ as
    $(-T)\in B(X,Y)$ such that it sends an element $x\in X$ to the additive
    inverse of $Tx \in Y$ then we have that
    \begin{align*}
        (T + (-T))x = Tx + (-T)x = 0 = \theta x
    \end{align*}
    So we can write that $T + (-T) = \theta$.
\end{proof}
\begin{proof}{\textbf{2}}
    Let $f,g$ be bounded linear functionals and $\alpha, \beta$ be nonzero
    scalars. Let $x,y \in \dom(f) \cap \dom(g)$ and let $\delta, \eta$ be
    scalars then we see that
    \begin{align*}
        h(\delta x + \eta y)
        &= \alpha f(\delta x + \eta y) + \beta g(\delta x + \eta y)\\
        &= \alpha (\delta f(x) + \eta f(y)) + \beta (\delta g(x) + \eta g(y))\\
        &= \delta (\alpha f(x) + \beta g(x)) + \eta (\alpha f(y) + \beta g(y))\\
        &= \delta h(x) + \eta h(y)
    \end{align*}
    This implies that $h$ is a linear operator.
    Also, since $\dom(h) = \dom(f) \cap \dom(g) \in X$ and $\range(h)$ is in
    the scalar field $K$ of $X$ (as $\range(f)$ and $\range(g)$ are)
    then $h$ is a linear functional.

    On the other hand, we know that $f$ and $g$ are bounded so there are two
    scalars $c,d$ such that
    \begin{align*}
        |f(x)| \leq c\|x\| \quad\text{and}\quad |g(x)| \leq d\|x\|
    \end{align*}
    for all $x \in \dom(f) \cap \dom(g)$. Also, we have that
    \begin{align*}
        |h(x)| = |\alpha f(x) + \beta g(x)|
        \leq |\alpha f(x)| + |\beta g(x)| = |\alpha||f(x)| + |\beta||g(x)|
    \end{align*}
    Then
    \begin{align*}
        |h(x)| \leq |\alpha||f(x)| + |\beta||g(x)|
        \leq |\alpha|c\|x\| + |\beta|d\|x\| = (|\alpha|c + |\beta|d)\|x\| 
    \end{align*}
    Therefore taking the constant $|\alpha|c + |\beta|d$ the linear functional
    $h$ is also bounded over the domain $\dom(f) \cap \dom(g)$.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{3}}
    Let $T_1,T_2$ be bounded linear operators and $\alpha, \beta$ be nonzero
    scalars. Let $x,y \in \dom(T_1) \cap \dom(T_2)$ and let $\delta, \eta$ be
    scalars then we see that
    \begin{align*}
        T(\delta x + \eta y)
        &= \alpha T_1(\delta x + \eta y) + \beta T_2(\delta x + \eta y)\\
        &= \alpha (\delta T_1(x) + \eta T_1(y)) + \beta (\delta T_2(x) + \eta T_2(y))\\
        &= \delta (\alpha T_1(x) + \beta T_2(x)) + \eta (\alpha T_1(y) + \beta T_2(y))\\
        &= \delta T(x) + \eta T(y)
    \end{align*}
    This implies that $T$ is a linear operator.

    On the other hand, we know that $T_1$ and $T_2$ are bounded so there are two
    scalars $c,d$ such that
    \begin{align*}
        \|T_1(x)\| \leq c\|x\| \quad\text{and}\quad \|T_2(x)\| \leq d\|x\|
    \end{align*}
    for all $x \in \dom(T_1) \cap \dom(T_2)$. Also, we have that
    \begin{align*}
        \|T(x)\| = \|\alpha T_1(x) + \beta T_2(x)\|
        \leq \|\alpha T_1(x)\| + \|\beta T_2(x)\| = |\alpha|\|T_1(x)\| + |\beta|\|T_2(x)\|
    \end{align*}
    Then
    \begin{align*}
        \|T(x)\| \leq |\alpha|\|T_1(x)\| + |\beta|\|T_2(x)\|
        \leq |\alpha|c\|x\| + |\beta|d\|x\| = (|\alpha|c + |\beta|d)\|x\| 
    \end{align*}
    Therefore taking the constant $|\alpha|c + |\beta|d$ the linear operator
    $T$ is also bounded over the domain $\dom(T_1) \cap \dom(T_2)$.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{6}}
    Let $X$ be the space of ordered $n$-tuples of real numbers with norm
    $\|x\| = \max_j |\xi_j|$ where $x = (\xi_1, ..., \xi_n)$.
    Then $f \in X'$ can be written as $f(x) = a_1\xi_1 + ... + a_n\xi_n$
    so we have that 
    \begin{align*}
        |f(x)| &= |a_1\xi_1 + ... + a_n\xi_n|
        \leq |a_1||\xi_1| + ... + |a_n||\xi_n| \leq\\
        &\leq |a_1|\max_j|\xi_j| + ... + |a_n|\max_j|\xi_j|
        = |a_1|\|x\| + ... + |a_n|\|x\|
    \end{align*}
    Then taking the supremum on both sides for any $x \in X$ such that
    $\|x\| = 1$ we get that
    \begin{align*}
        \sup_{\substack{x \in X \\ \|x\| = 1}} |f(x)| &\leq |a_1| + ... + |a_n|
    \end{align*}
    But from the first inequality we see that if we take $x$ such that
    $\xi_j = \pm 1$ such that $a_j\xi_j \geq 0$ we get that 
    \begin{align*}
        |f(x)| &= |a_1 + ... + a_n| = |a_1| + ... + |a_n|
    \end{align*}
    Therefore we found $x$ such that 
    \begin{align*}
        \sup_{\substack{x \in X \\ \|x\| = 1}} |f(x)| = |a_1| + ... + |a_n|
    \end{align*}
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{8}}
    A Schauder basis for $c_0$ is $(e_k)$ where $e_k = (\delta_{kj})$.
    Let $x \in c_0$ then we can write $x$ as
    \begin{align*}
        x = \sum_{k=1}^\infty \xi_k e_k
    \end{align*}
    Also, let $f \in c_0'$ where $c_0'$ is the dual space of $c_0$.
    Since $f$ is linear and bounded we have that
    \begin{align*}
        f(x) = \sum_{k=1}^\infty \xi_k \gamma_k
    \end{align*}
    Where $\gamma_k = f(e_k)$. Now, let $x_n = (\xi_k^{(n)}) \in c_0$ defined as 
    \begin{align*}
        \xi_k^{(n)} = \begin{cases}
            -1 &\text{ if } \gamma_k < 0 \text{ and } k \leq n\\
            1 &\text{ if } \gamma_k > 0 \text{ and } k \leq n\\
            0 &\text{ if } \gamma_k = 0 \text{ or } k > n
        \end{cases}
    \end{align*}
    Then we have that
    \begin{align*}
        |f(x_n)|
        = \bigg|\sum_{k=1}^n \xi_k^{(n)}\gamma_k\bigg|
        = \bigg|\sum_{k=1}^n \text{sign}(\gamma_k)\gamma_k\bigg|
        = \sum_{k=1}^n |\gamma_k|
        \leq \|f\|\|x_n\| = \|f\|
    \end{align*}
    Where we used that $\|x_n\| = \sup_j \xi_j^{(n)} = 1$. Since $n$ is
    arbitrary, letting $n \to \infty$ we obtain that
    \begin{align*}
        \sum_{k=1}^\infty |\gamma_k| \leq \|f\|
    \end{align*}
    This shows that $(\gamma_k) \in l^1$.
    
    Conversely, let $b = (\beta_k) \in l^1$ we can get a corresponding bounded
    linear functional $g$ on $c_0$. In fact, we may define $g$ on $c_0$ by
    \begin{align*}
        g(x) = \sum_{k=1}^\infty \xi_{k}\beta_k
    \end{align*}
    Where $x = (\xi_k)\in c_0$. Then $g$ is linear, also we see that
    \begin{align*}
        |g(x)| = \bigg|\sum_{k=1}^\infty \xi_{k}\beta_k\bigg|
        \leq \sum_{k=1}^\infty |\xi_{k}||\beta_k|
        \leq \sup_j |\xi_j| \sum_{k=1}^\infty |\beta_k|
        \leq \|x\| \sum_{k=1}^\infty |\beta_k|
    \end{align*}
    But $\sum_{k=1}^\infty |\beta_k| < \infty$ then $g$ is bounded and hence
    $g \in c_0'$.

    Finally, we show that the norm of $f$ is the norm on the space $l^1$.
    We see that
    \begin{align*}
        |f(x)|
        \leq \sum_{k=1}^\infty |\xi_{k}||\gamma_k|
        \leq \sup_j |\xi_j| \sum_{k=1}^\infty |\gamma_k|
        \leq \|x\| \sum_{k=1}^\infty |\gamma_k|
    \end{align*}
    Then taking the supremum over all $x$ of norm 1 we obtain
    \begin{align*}
        \|f\| \leq \sum_{k=1}^\infty |\gamma_k|
    \end{align*}
    But also we saw that $\sum_{k=1}^\infty |\gamma_k| \leq \|f\|$
    then must be that
    \begin{align*}
        \|f\| = \sum_{k=1}^\infty |\gamma_k|
    \end{align*}
    This can be written as $\|f\| = \|c\|_1$, where $c = (\gamma_k) \in l^1$
    and $\gamma_k = f(e_k)$.
    The mapping of $c_0'$ onto $l^1$ defined by $f \to c$ is linear and
    bijective, and it is norm preserving, so it is an isomorphism.
\end{proof}
\cleardoublepage
\begin{proof}{\textbf{12}}
    We know that if $Y$ is a Banach space, then $B(X,Y)$ is a Banach space.
    Where $B(X,Y)$ is the set of all bounded linear operators from $X$ into $Y$.
    
    In the case of bounded linear functionals we have that $Y = \R$ then since
    $\R$ is a Banach space we have that $B(X, \R)$ is a Banach space.
    
    The dual space of $\R^n$ is $B(\R^n, \R) = \R^n$ which implies that $\R^n$
    is a Banach space and hence complete.

    The dual space of $l^1$ is $B(l^1, \R) = l^\infty$ which implies that
    $l^\infty$ is a Banach space and hence complete.

    Finally, the dual space of $l^p$ is $B(l^p, \R) = l^q$ which implies that
    $l^q$ is a Banach space and hence complete.
\end{proof}


\end{document}